import os
import sys
sys.path.append('..')
import utils.model as model_utils

from math import exp
from typing import List
from tqdm import tqdm
import urllib
import json
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

import transformers
import torch
FORCE_RESET = bool(int(os.getenv("FORCE_RESET", "0")))

from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import bpe
import gc
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"



class PPL_calculator:
    def __init__(self, model='gpt2', device = "cuda", max_length=None,  stride=None):
        self.model_id = model
        assert 'gpt2' in model, "Only GPT-2 models are supported"
        self.device = device
        self.model = GPT2LMHeadModel.from_pretrained(self.model_id).to(device)
        self.model.eval()

        self.tokenizer = GPT2TokenizerFast.from_pretrained(self.model_id)
        self.max_length = max_length if max_length is not None else 1024
        self.stride = stride if stride is not None else self.max_length

    def get_perplexity(self, text: str) -> float:
        # Tokenize the input text
        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)
        seq_len = input_ids.size(1)

        nlls = []
        prev_end_loc = 0
        for begin_loc in range(0, seq_len, self.stride):
            end_loc = min(begin_loc + self.max_length, seq_len)
            trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
            input_ids_piece = input_ids[:, begin_loc:end_loc].to(self.device)
            target_ids = input_ids_piece.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids_piece, labels=target_ids)

                # loss is calculated using CrossEntropyLoss which averages over valid labels
                # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
                # to the left by 1.
                neg_log_likelihood = outputs.loss

            nlls.append(neg_log_likelihood)

            prev_end_loc = end_loc
            if end_loc == seq_len:
                break

            del input_ids_piece, outputs, neg_log_likelihood

        ppl = torch.exp(torch.stack(nlls).mean()).item()
        del nlls
        gc.collect()
        torch.cuda.empty_cache()
        return ppl
                   


class Defender:
    PPL_UPPER_BOUND = 175.57
    PPL_REFUSAL_STRING = "Sorry, but I can not assist with that."

    PARA_INST = "Please paraphrase the following sentences. Give me paraphrased results only. Do not include any other information.\n"

    def __init__(self,name=None, retoken_dropout_rate=0.2, merge_table_path='./subword_nmt.voc', para_model='gpt-3.5-turbo-0613') -> None:
        self.name = name
        if name == 'ppl':
            self.ppl_calculator = PPL_calculator()
            self.handler = self.ppl_def
        elif name == 'para': # paraphrase
            self.handler = self.para_def
            self.para_model = model_utils.load_model(para_model, 0)

        elif name == 'retok': # re tokenization
            self.handler = self.retok_def
            merge_table = bpe.load_subword_nmt_table(merge_table_path)
            self.re_tokenizer = bpe.BpeOnlineTokenizer(
                        bpe_dropout_rate = retoken_dropout_rate,
                        merge_table = merge_table)

            
        else:
            self.handler = self.no_defense

    def no_defense(self, model, instruction, debug=False):
        response = model(instruction, n=1, debug=debug)
        return model.resp_parse(response)[0] 
    
    def ppl_def(self, model, instruction):
        ppl = self.ppl_calculator.get_perplexity(instruction)
        if ppl > self.PPL_UPPER_BOUND:
            return self.PPL_REFUSAL_STRING
        else:
            return self.no_defense(model, instruction)
        
    def para_def(self, model, instruction):
        para_instruction = self.para_model(self.PARA_INST + instruction, n=1, debug=False)
        para_instruction = self.para_model.resp_parse(para_instruction)[0]
        
        return self.no_defense(model, para_instruction)

    def retok_def(self, model, instruction):
        instruction = self.re_tokenizer(instruction, 
                                        sentinels=['', '</w>'],
                                        regime='end',
                                        bpe_symbol=' ')
        
        return self.no_defense(model, instruction)

    def end_handler(self):
        if self.name == 'ppl':
            self.ppl_calculator.model = None
            self.ppl_calculator.tokenizer = None
            
        elif self.name == 'retok':
            self.re_tokenizer = None
            
        gc.collect()
        torch.cuda.empty_cache()
        return
        
        